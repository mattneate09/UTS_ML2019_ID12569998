{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " A1 submission.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattneate09/UTS_ML2019_ID12569998/blob/master/A1_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZYZXfTqaIrr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Machine Learning \n",
        "Assignment 1\n",
        "Matthew Neate\n",
        "12569998\n",
        "\n",
        "\n",
        "https://github.com/mattneate09/UTS_ML2019_ID12569998\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Content:\n",
        "Neural Networks have become a point of interest due to their incredible accuracy and capacity to solve niche problems. The paper investigates improvements on text recognition. Specifically, on neural network architectures and their advantages over alternate approaches. Neural networks are complex algorithms that benefit greatly from recent advances in hardware, but more importantly, they benefit from the rich and wide sources of data that can be used for training. Text recognition is now a somewhat simple task due to these algorithms.  The paper explains the fundamental concepts of neural networks. Focusing first on gradient descent. Which is the attempt to minimize the error of a complex, multi-dimensional system. This is done by ensuring the error function is differentiable, and then testing points on the error function (via differentiation) to find a new point that is likely to be “lower” (result in less errors) than the previous point. Finding this new point is done by changing the weights of the neurons. The neurons act as signal dampeners or amplifiers from the input signal. These steps are consecutively repeated until an acceptable minimum is found. The simplest equation for this weight adjustment process is: \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Or simply, the current weight = the previous weight shifted in the direction of the gradient of the loss function with respect to the parameters (weights).  A simple 2D graph can help visualize the fundamental process.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A very common method of achieving this is via a process called back propagation. It is undertaken by working backwards through the network, changing weights from the output layers and through to the input layers. \n",
        "Convolutional networks are used to great success due to the ability for the network to extract “features” that are convolutions (hence the name) of the original inputs. These features can be edges, corners or much more abstract objects that the network uses to identify separate classes. The convolutions are created using a “kernal”, which takes groups of pixels as an input to feed to subsequent layers in the network. \n",
        "\n",
        "\n",
        "The process of feature extraction greatly increases the effectiveness of neural networks in image recognition tasks.  Specifically, in text recognition, as the network can look at edges, corners and shapes of the input text. This minimizes the effect of noise, skew and shift on the output as the network is not looking for specific pixels to be lit up, rather, the network is looking for multiple combinations of pixels that form parts of the shape that resemble specific characters.  The network is even able to be trained on inputs that are distorted to help identify the features that describe each class.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The complexity of both computational power and data resources required for the proper utilization of convolutional neural networks are the major underpinning of their performance for complex tasks.  The paper uses model performance on text recognition tasks as a benchmark demonstrating the significant advantages of neural models over other classifiers.\n",
        " \n",
        "\n",
        "\n",
        "Innovation:\n",
        "The Paper demonstrates significant improvements on text recognition through the utilization of complex neural network models.  Published in 1998, the work presented is still extremely relevant and even misunderstood by the general public.  Text recognition is currently seen as a somewhat trivial problem. Despite this, the paper remains relevant by clearly demonstrating the application of neural networks as the most viable solution for text recognition, setting the stage for larger problems to be solved. The paper provides multiple comparisons of the networks outperforming other models, including SVMS and K nearest neighbours. The paper really innovates by expanding further into the use of gradient descent in the application of graph transformer networks. It demonstrates that gradient based learning is not limited to networks containing simple modules with fixed sized vectors, rather that it can be generalized and abstracted to form graph transformer networks. Whereby inputs and outputs are dynamic graphs, and gradient learning can be applied if the data used to create the graph is differentiable. These multi-modular systems are capable of performing multiple complex tasks. The simplest form of this demonstrated in the paper is a system that can be trained to simultaneously segment and recognize words. The researchers demonstrate and explain their GTN model that was used by multiple banks across the US to read and verify millions of cheques.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This model shows multiple graph transformer modules. The system is able to read higher levels of information due to the transformations of the previous transformer “layer”.  This system follows these steps: recognizes where text is, segments text, recognizes characters, composes a set of characters to meaningful objects i.e words or amounts and then produces the most meaningful result. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Technical Quality:  \n",
        "The paper builds upon previous works from the authors and others. It builds up to the idea of graph transfer networks by explaining the underlying concepts and performing a comparison of models. It does this rather well, as multiple performance indexes are compared, including overall error rates, rejection performance, number of operations needed to classify a class, and memory requirements. These are strong candidates for accurate comparison of models. The memory requirements comparison is measured by number of variables, which may be slightly misleading. However, he authors state that most variables are only one byte, so this affect is probably negligible.  Graph transformer networks are quite difficult to understand, and the paper does not go into significant description of how to create such networks. I had to refer to previous papers by the authors to understand the concept of a GTN in detail. Because of this it would be hard to replicate some of the findings that the researchers found. This is especially true as the paper presents an entire solution to document reading which would take some time to recreate. This is likely the case for many niche machine learning papers. As the topic is fundamentally complex and requires a solid understanding in mathematics, statistics and programming. Overall the technical quality of the paper was very good, with ample descriptions and support (albeit complex) backing up the authors claims.\n",
        "\n",
        "\n",
        "Application and X-Factor:\n",
        "The paper demonstrates neural networks utilizing gradient descent, and compares them to other methods for character recognition. The researchers then further develop a Graph transformer network, which is widely successful in the multi stage process of character recognition of bank cheques. They demonstrate successful new advances in the domain of character recognition, by integrating learning via gradient descent into a systematic pipeline that was used widely throughout industry. The work could be applied to speech recognition too, as the nature of graph transformer networks allows them accept graphs as input and outputs. These objects are not limited to set boundaries like vector and matrix sizes. This will make them useful in speech detection as the nature of speech is that it is not arbitrarily bound by length. Speech detection will also require multiple steps of splitting the waveform into words and characters/phonemes. This would thus likely require a similar solution in the form of a graph transformer network. The idea of a network that takes graphs as inputs and outputs would likely form a good discussion in class, as the notion is rather tricky to understand. GTN’s are quite interesting due to their ability to change dynamically to solve a give problem.  Having a descent understanding of neural models already, this next layer of complexity sparks curiosity and insight to how more complex neural models are developed and deployed.\n",
        "\n",
        "\n",
        "Presentation:\n",
        "This paper seemed pretty standard for an academic paper. It was dense in information, but laid out a significant amount of background knowledge before commencing into the real heart of discussion. The initial background information was quite easy to follow along logically. However GTN’s were introduced a bit too hastily, as little explanation about the information transfer occurred. Specifically the paper states that graphs are used to transfer data between the network instead of vectors and matrixes. The issue I had with this is that a conceptual and sound description of this meant was lacking. This is understandably hard to explain. I Did find that the authors previous paper specifically on GTN’s was more effective at describing them.  The authors did go into strong details about other aspects of GTN’s, such as application and design. Overall this was a clear and in-depth paper that was presented quite well. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}